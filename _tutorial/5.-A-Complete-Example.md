---
layout: page
title: 5.A-Complete-Example
description: "A-Complete-Example"
tags: [tutorial]
modal-id: A-Complete-Example
---

## A RNA Seq data processing workflow

If you are curious about how to write large workflows using SoS or just how a long SoS script looks like, here is a RNA Seq data processing workflow written in SoS. The workflow consists of many steps and was written in a mixed style, namely a spine of forward-moving steps and several auxiliary steps that supply required information. The DAG of the workflow looks like
 

![example_dag.png]({{site.baseurl}}/media/example_dag.png){:class="img-responsive"}

although this DAG does not include many auxiliary steps (e.g. those that download required resources) because these steps were called once and would not be called once the resources are available. The DAG is generated using command

```
sos prepare RNASeq_hg10.sos
cat .sos/human_hg19.dot | dot -T png > example_dag.png
```

This tutorial will divide the workflow into several parts and briefly describe each of them.

### Header

The header of script contains description of the workflow.
{% highlight sos %}

#!/usr/bin/env sos-runner
#fileformat=SOS1.0

# Workflow for a comprehensive analysis of paired-end RNA seq data
# that performs read-level quality assessment, alignment, alignment quality
# assessment, summary statistics, fusion detection and filtering, gene and exon
# level expression counts, and variant calling. The input of this pipeline
# should be a list of fastq or fastq.gz files with paired reads differentiated
# by _R1_ and _R2_ in filenames. <p>The output of the pipeline should be a
# directory to which all results will be written. The directory will be 
# created if it does not exist. Note that it is highly recommended that
# you execute the pipeline with a subset of samples (using option --sampling
# 1000000) to test the pipeline before you apply it to the full dataset. 
# Network connection will be needed for the first run (to download resources)
# but not for subsequent runs. For details of the pipeline including the
# accepted parameters, please run command "vtools show pipeline RNASeq_hg19_v3".

# human_hg19
# 
# This pipeline uses the hg19 version of the reference
# genome, genes downloaded from Illumina iGenome, and existing variants from
# the GATK resource bundle. This pipeline uses <ul>
#   <li>tophat 2.0.13 
#   <li>bowtie 1.1.1
#   <li>samtools 0.1.19
#   <li>picard 1.82
#   <li>GATK 3.3
#   <li>fastqc 0.11.2
#   <li>BEDTools 2.17.0
#   <li>RSeQC 2.4, and
#   <li>Oncofuse 1.0.7
#   </ul>
# Other versions of the tools can be used if you execute the pipeline with option
# --strict_version False, but the pipeline might or might not work as expected. 
# Please download all these tools and make them available to the pipeline (set $PATH),
# and start the pipeline using command "vtools execute /path/to/this_file options".
# Noted that the precompiled version of tophat 2.0.13 can fail under mac so it is
# recommended that you compile tophat2 from source.

```

### inclusion of auxiliary steps

```
%from install_ngs include *
```

### Global definitions

```

if os.path.isdir(os.path.expanduser('~/rsrch1/variant_tools_resource/pipeline_resource')):
    resource_dir  = os.path.expanduser('~/rsrch1/variant_tools_resource/pipeline_resource')
elif os.path.isdir(os.path.expanduser('~/.variant_tools/pipeline_resource')):
    resource_dir  = os.path.expanduser('~/.variant_tools/pipeline_resource')
else:
    fail_if(True, 'Unable to locate a resource directory')

# Path to picard jar files
parameter: picard_path = '~/bin/Picard'

# Path to GenomeAnalysisTK.jar
parameter: gatk_path = '~/bin/GATK'

# Path to Oncofuse.jar
parameter: oncofuse_path = '~/bin/Oncofuse'

# input fastq files
parameter: fastq_files = list

fail_if(len([x for x in fastq_files if '_R1_' in x]) == 0  \
    or len([x for x in fastq_files if '_R1_' in x]) != len([x for x in fastq_files if '_R2_' in x]),
    'Input file names are not paired fastq files differentiated by _R1_ and _R2_ in filename.')

# Name of the sample being processed, which will be used for filename
# and read group of aligned reads.
parameter: samplename = str

# If set to a positive number, this pipeline will sample specified
# number of reads (pairs) from the input. This is useful to test if the pipeline
# works for your environment, download all needed resources, created indexes,
# and check the quality of data.
parameter: sampling = 0


# An optional filename that contains detailed information
# about the sample. The sample sheet should be a tab or comma-delimited 
# file with a header, and one or more lines with one of which contain the 
# information of the sample. The information from the sample sheet will be
# written to the final report (summary.html).
parameter: sample_sheet = ''

#
# CONSTANT VALUES: not configurable in this pipeline
# 
pipeline_name     = 'RNASeq_hg19'
pipeline_version  = 'ver1.0 (2016/8/29'
reference_genome  = 'hg19'
reference_genes   = 'UCSC/Illumina iGenome'
# HG19 version of the resource files
gatk_resource_dir = '${resource_dir}/GATK/2.8/hg19/'
igenome_resource_dir = '${resource_dir}/iGenome'
ncbi_resource_dir = '${resource_dir}/NCBI'

igenome_url       = 'ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Homo_sapiens/UCSC/hg19/Homo_sapiens_UCSC_hg19.tar.gz'
gene_info_url     = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'
reference_dir     = '${igenome_resource_dir}/Homo_sapiens/UCSC/hg19/Sequence'
transcriptome_dir = '${igenome_resource_dir}/Homo_sapiens/UCSC/hg19/Annotation/transcriptome_index'
annotation_dir    = '${igenome_resource_dir}/Homo_sapiens/UCSC/hg19/Annotation'
refgene_txt       = '${annotation_dir}/Genes/refGene.txt'
genes_gtf         = '${annotation_dir}/Genes/genes.gtf'
# generated files'
refgene_bed       = '${annotation_dir}/Genes/refGene.bed'
genes_exon_bed    = '${annotation_dir}/Genes/genes_exon.bed'
genes_with_chr_gtf= '${annotation_dir}/Genes/genes_chr.gtf'
genesize_txt      = '${annotation_dir}/Genes/geneSize.txt'
# Download from UCSC'
ensgene_url       = 'http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/ensGene.txt.gz'
ensgene_txt       = '${annotation_dir}/Genes/ensGene.txt'
chromsize_url     = 'ftp://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/chromInfo.txt.gz'
chromsize_txt     = '${annotation_dir}/chromsize.txt'
# GATK resource bundle'
gatk_url          = 'ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/2.8/hg19/'
# '
# Output directories'
output_dir        = samplename
temp_dir          = '${output_dir}/tmp'
alignment_out     = '${output_dir}/alignment'
alignmentqc_out   = '${output_dir}/alignment_qc'
fusion_out        = '${output_dir}/fusion'
readqc_out        = '${output_dir}/read_qc'
count_out         = '${output_dir}/counts'
variant_out       = '${output_dir}/variants'



[human_hg19_100: skip=sampling == 0, shared='fastq_files']
# Select a subset of reads if parameter --sampling is specified. Output will be 
# written to the project cache directory.
input:
    fastq_files

depends:
    executable('gunzip')

output:
    '${temp_dir}/${samplename}_R1_${sampling}.fastq',
    '${temp_dir}/${samplename}_R2_${sampling}.fastq'

run:
    gunzip -c ${' '.join(sorted([x for x in input if '_R1_' in x]))} | head -n ${4*sampling} > ${output[0]}
    gunzip -c ${' '.join(sorted([x for x in input if '_R2_' in x]))} | head -n ${4*sampling} > ${output[1]}

fastq_files = output


[human_hg19_110]
# Use fastqc to check the quality of input reads
#
input:
    fastq_files

depends:
    executable('fastqc'),
    executable('gunzip')

output:
    '${readqc_out}/${samplename}_fastqc.html',
    '${readqc_out}/${samplename}_fastqc.zip'

task:

run:
    # the input can be original .fast.gz file or extracted 
    ${"gunzip -c" if input[0].endswith(".gz") else "cat"} ${input} | fastqc stdin --outdir=${readqc_out}
    mv ${readqc_out}/stdin_fastqc.html  ${output[0]}
    mv ${readqc_out}/stdin_fastqc.zip ${output[1]}

[extract_fastqc_figures: provides=[os.path.join(readqc_out, x) for x in ('per_base_quality.png', 'per_sequence_gc_content.png', 'per_base_sequence_content.png')]]
input:
    '${readqc_out}/${samplename}_fastqc.zip'

import zipfile
import os
with zipfile.ZipFile(input[0]) as qczip:
    for name in qczip.namelist():
        if os.path.basename(name) in ['per_base_quality.png', 'per_sequence_gc_content.png', 'per_base_sequence_content.png']:
            qczip.extract(name, readqc_out)
            # move the file to top directory
            os.rename(os.path.join(readqc_out, name), os.path.join(readqc_out, os.path.basename(name)))


[download_ensgene: provides=genes_gtf]
download:  '${annotation_dir}/Genes/'
    ${ensgene_url}

[chr_gtf: provides = genes_with_chr_gtf]
# takes gtf and and chromosome names to gene name to avoid problems
# with genes with the same names on different chromosomes
input:
    genes_gtf

output:
    genes_with_chr_gtf

python:
    #
    #    Input format:
    #
    #        chr1 unknown exon 17233 17368 . - . gene_id "WASH7P"; gene_name "WASH7P"; transcript_id "NR_024540"; tss_id "TSS8151";
    #
    #    Output format:
    #           chr1 unknown exon 17233 17368 . - . gene_id "WASH7P"; gene_name "WASH7P.chr1"; transcript_id "NR_024540"; tss_id "TSS8151";
    # 
    import re
    pattern = re.compile(r'''(gene_id\s+['"]*)([\w.-]+)''')
    # only work on selected chromosomes
    allowed_chromosomes = set([str(x) for x in range(25)] + ['chr{}'.format(x) for x in range(25)] + \
                ['X', 'Y', 'M', 'chrX', 'chrY', 'chrM'])
    with open(input[0], 'rU') as ifile, open(output[0], 'w') as ofile:
       for line in ifile:
           if line.startswith('#'):
               continue
           ls = line.strip().split('\t')
           if ls[0] not in allowed_chromosomes:
               continue
           ofile.write(re.sub(pattern, r'\1\2.{}'.format(ls[0]), line)) 

[download_igenome: provides=refgene_bed]
download: dest_dir=igenome_resource_dir
    ${igenome_url}

[human_hg19_400: shared={'param': 'output[0]'}]
#
# Align a subset of reads to obtain two key parameters parameters
# --mate-inner-dist and --mate-std-dev for full scale alignment
#
input:
    fastq_files

depends:
    genes_with_chr_gtf,
    refgene_bed,
    executable('tophat2'),
    executable('bowtie'),
    executable('inner_distance.py')
    
output:
    '${temp_dir}/${samplename}.inner_distance_freq.txt'

test_files = '${temp_dir}/${samplename}_R1_test.fastq', '${temp_dir}/${samplename}_R2_test.fastq'

task:

run:
    ${'gunzip -c' if input[0].endswith('.gz') else 'cat'}  ${' '.join(sorted([x for x in input if '_R1_' in x]))} \
        | head -n 40000 > ${test_files[0]}

    ${'gunzip -c' if input[0].endswith('.gz') else 'cat'}  ${' '.join(sorted([x for x in input if '_R2_' in x]))} \
        | head -n 40000 > ${test_files[1]}

run:
    tophat2 --zpacker 0 -no-coverage-search --library-type fr-unstranded \
        --max-multihits 20 --prefilter-multihits --num-threads 8 \
        --GTF ${genes_with_chr_gtf} --segment-length 25 \
        --transcriptome-index ${transcriptome_dir}/genes --bowtie1 \
        --output-dir ${temp_dir} ${reference_dir}/BowtieIndex/genome \
        ${test_files[0]} ${test_files[1]}

run:
    inner_distance.py -r ${refgene_bed} -i ${temp_dir}/accepted_hits.bam -o ${temp_dir}/${samplename}


[human_hg19_440: shared={'accepted_hits':'output[0]', 'mate_inner_dist':'mate_inner_dist', 'mate_std_dev':'mate_std_dev'}]
#Alignment (Tophat2): Running tophat2 with fusion search.
#Alignment Preparation: Calculate mean and standard devision of inner distance and set parameters
# --mate-inner-dist and --mate-std-dev
import math
values = []
with open(param) as freq:
    for line in freq:
        s, e, c = [int(x) for x in line.split()]
        values.extend([int(s+e)/2.] * c)
if not values:
    # no mapped reads...
    mate_inner_dist = 50
    mate_std_dev = 20
    logger.info('Mean and standard deviation of inner distance is set to default values 50 and 20')
else:
    mean = sum(values)/len(values)
    mean_sq = sum([x*x for x in values])/len(values)
    sd = math.sqrt(mean_sq - mean*mean)
    mate_inner_dist = int(mean)
    mate_std_dev = int(sd)
    logger.info('Mean and standard deviation of inner distance is set to {} and {}'.format(int(mean), int(sd)))

input:
    fastq_files

depends:
    genes_with_chr_gtf

output:
    '${alignment_out}/accepted_hits.bam'

# use --zpacker 0 to disable compression of temporary file
# which makes the program run faster
run:
    if [[ -f ${alignment_out}/logs/run.log ]] && [[ ! -f ${alignment_out}/accepted_hits.bam ]];  \
        then  \
        tophat2 --resume ${alignment_out} ||  \
        tophat2  \
        -no-coverage-search  \
        --library-type fr-unstranded  \
        --max-multihits 20  \
        --prefilter-multihits  \
        --num-threads 8  \
        --GTF ${genes_with_chr_gtf}  \
        --segment-length 25  \
        --keep-fasta-order  \
        --transcriptome-index ${transcriptome_dir}/genes  \
        --fusion-search --fusion-ignore-chromosomes chrM,M  \
        --fusion-min-dist 50000  \
        --bowtie1  \
        --mate-inner-dist ${mate_inner_dist}  \
        --mate-std-dev ${mate_std_dev}  \
        --output-dir ${alignment_out}  \
        ${reference_dir}/BowtieIndex/genome  \
        ${','.join(sorted([x for x in input if '_R1_' in x]))}  \
        ${','.join(sorted([x for x in input if '_R2_' in x]))}  \
        ;  \
    else   \
        tophat2  \
        -no-coverage-search  \
        --library-type fr-unstranded   \
        --max-multihits 20   \
        --prefilter-multihits  \
        --num-threads 8   \
        --GTF ${genes_with_chr_gtf}  \
        --segment-length 25   \
        --keep-fasta-order   \
        --transcriptome-index ${transcriptome_dir}/genes  \
        --fusion-search --fusion-ignore-chromosomes chrM,M   \
        --fusion-min-dist 50000  \
        --bowtie1   \
        --mate-inner-dist ${mate_inner_dist}  \
        --mate-std-dev ${mate_std_dev}  \
        --output-dir ${alignment_out}  \
        ${reference_dir}/BowtieIndex/genome  \
        ${','.join(sorted([x for x in input if '_R1_' in x]))}  \
        ${','.join(sorted([x for x in input if '_R2_' in x]))};  \
        fi 

 
[index_bam: provides='{filename}.bam.bai']
# Index generated bam file
input:
    '${filename}.bam'

run:
    samtools index ${input}


[human_hg19_460]
# Add ReadGroup information to sorted bam file.

# Read group identifier, which should be an unique identifier for each
# instrument run (e.g. flowcell number + lane name + number). The filename before
# the '_R1_' part will be used if this option is left unspecified.
parameter: rgid = 'NA'
warn_if(rgid == 'NA',
    'A default (NA) value is used for parameter --rgid (intrument run ID)')

# Platform/technology used to produce the read. Valid values include
# ILLUMINA, SOLID, LS454, HELICOS and PACBIO
parameter: rgpl = 'ILLUMINA'
# check input parameters
warn_if(not rgpl in ['ILLUMINA', 'SOLID', 'LS454', 'HELICOS', 'PACBIO'],
    'PL (platform) value if not one of ILLUMINA, SOLID, LS454, HELICOS and PACBIO')

# DNA preparation library identity.
parameter: rglb = 'NA'
warn_if(rglb == 'NA',
    'A default (NA) value is used for parameter --rglb (library identification string)')
#
# Platform unit (e.g. barcode). This field is not used by GATK.
parameter: rgpu = 'NA'
warn_if(rgpu == 'NA',
    'A default (NA) value is used for parameter --rgpu (platform unit, barcode)')


input:
     accepted_hits

output:
    '${temp_dir}/${samplename}-with_rg.bam'

run:
    java -Xmx16g -Xms512m   \
        -Djava.io.tmpdir=${temp_dir}   \
        -jar ${picard_path}/AddOrReplaceReadGroups.jar  \
        input=${input}  \
        OUTPUT=${temp_dir}/${samplename}-with_rg.bam  \
        VALIDATION_STRINGENCY=LENIENT  \
        RGID=${rgid}  \
        RGSM=${samplename}  \
        RGPL=ILLUMINA  \
        RGLB=${rglb}  \
        RGPU=${rgpu}  \
        RGCN=IPCT

[human_hg19_470: shared={'sorted_unique':'output[0]'}]
#Alignment: Remove fusion reads in preparation for variant calling.
# step 475, samtools index is defined above
output:
    '${alignment_out}/${samplename}-sorted_uniq_nonF.bam'

run:
    samtools view -h ${input} \
        | awk -F '\t' '{ if($0 ~ "^@") {print} else { for(i=12;i<=NF;i++){ if ($i ~ "NH:i:1$"){print}} } }' \
        | awk '{ if($0 ~ "^@") {print} else { if ($0 !~ "XF:Z") {print} }  }' \
        | samtools view -bS - > ${alignment_out}/${samplename}-sorted_uniq_nonF.bam


[human_hg19_480]
#Alignment: Count the number of junction reads. 
input:
    accepted_hits

output:
    '${alignment_out}/junction.count'

run:
    samtools view -h ${input} \
   | awk '$6 ~/N/' | awk '{ if ($9 ~ /^-/) {print $1"\t-"} else print $1"\t+" }' \
   | sort -T ${temp_dir} -u | wc -l > ${alignment_out}/junction.count


[human_hg19_500 (gene body coverage): shared={'rseqc_pdf':'output'}]
# this version uses BAM files to count geneBody coverage, using .bw might be a more resource
# saving idea but needs more testing, especially because the output of bigWig is currently normalized
# and does not show the correct depth of coverage.
input:
    sorted_unique

depends:
    refgene_bed

output:
    '${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.pdf'

# this job can take a long time to execute so we tend to execute as a separate job
task:
run:
    geneBody_coverage.py -r ${refgene_bed} -i ${input} -o ${alignmentqc_out}/${samplename}

#[human_hg19_560]
#RSeQC: Coverage of gene body
#RunCommand('geneBody_coverage2.py -r ${refgene_bed} -i ${input551: input551[0][:-3] + "bw"} -t pdf -o ${alignmentqc_out}/${samplename}',
#    output='${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.pdf')

[human_hg19_510]
#RSeQC: Check if current sequencing depth is deep enough to perform alternative splicing analyses
#RSeQC: Annotating splicing event and splicing junction.
#RSeQC: Plot the distribution of inner distance between paired reads, which can be used to 
#   determine parameters --mate-inner-dist and --mate-std-dev of the pipeline.
#RSeQC: read duplication plot
#RSeQC: Read distribution (statistics)
#RSeQC: Generate scaled wig track for depth
input:
    accepted_hits

output:
    '${alignmentqc_out}/${samplename}.junctionSaturation_plot.pdf',
    '${alignmentqc_out}/${samplename}.splice_events.pdf',
    '${alignmentqc_out}/${samplename}.splice_junction.pdf',
    '${alignmentqc_out}/${samplename}.inner_distance_plot.pdf',
    '${alignmentqc_out}//${samplename}.DupRate_plot.pdf',
    '${alignmentqc_out}//${samplename}.read_distribution'

task:
run:
    junction_saturation.py  -r ${refgene_bed} -i ${input} -o ${alignmentqc_out}/${samplename}
    junction_annotation.py -r ${refgene_bed} -i ${input} -o ${alignmentqc_out}/${samplename}
    inner_distance.py -r ${refgene_bed} -i ${input} -o ${alignmentqc_out}/${samplename}
    read_duplication.py -i ${input} -o ${alignmentqc_out}/${samplename}
    read_distribution.py -r ${refgene_bed} -i ${input} > ${alignmentqc_out}/${samplename}.read_distribution

[human_hg19_600 (SortSam): shared={'sorted_bam':'output'}]
#Count: Order reads by name so that the BAM file can be processed by htseq-count
input:
    accepted_hits

output:
    '${alignment_out}/${samplename}-sorted_by_id.bam'

task:
run:
    java -Xmx16g -Xms512m   \
        -Djava.io.tmpdir=${temp_dir}  \
        -jar ${picard_path}/SortSam.jar  \
        input=${input}  \
        OUTPUT=${alignment_out}/${samplename}-sorted_by_id.bam  \
        SO=queryname   \
        MAX_RECORDS_IN_RAM=1000000   \
        TMP_DIR=${temp_dir}  \
        VALIDATION_STRINGENCY=SILENT


[human_hg19_610 (htseq-count)]
# Running htseq-count to count reads for genes. This step uses the union
#    intersection mode, and a gene definition file with chromosome name added to gene_id.
input:
    sorted_bam

output:
    '${count_out}/${samplename}-gene_counts.tsv'

# We cannot use --order=pos because htseq-count would fail with an error message 
run:
    htseq-count --stranded=no --format=bam --type=exon \
     --mode=union ${input} ${genes_with_chr_gtf} \
     > ${count_out}/${samplename}-gene_counts.tsv


[get_genesize: provides=genesize_txt]
#    Genes.gtf file in the format of
#
#    chr1    unknown    exon    17233    17368    .    -    .    gene_id "WASH7P"; gene_name "WASH7P"; transcript_id "NR_024540"; tss_id "TSS8151";
#    chr1    unknown    exon    17369    17436    .    -    .    gene_id "MIR6859-2"; gene_name "MIR6859-2"; transcript_id "NR_107062"; tss_id "TSS24460";
#    chr1    unknown    exon    17369    17436    .    -    .    gene_id "MIR6859-1"; gene_name "MIR6859-1"; transcript_id "NR_106918_2"; tss_id "TSS24460";
#
# output (string):
#    A text file in the format of
#
#    chr1  MIR6859-2  112324
input:
    genes_gtf


python:

    import sys
    def _size(features):
        # this is definitely not efficient but we do not really care because it will be run
        # only once. 
        bases = set()
        for feature in features:
            # Add 1 because GTF input is one based
            bases.update(range(int(feature[0]), int(feature[1]) + 1))
        return len(bases)

    # only work on selected chromosomes
    allowed_chromosomes = set([str(x) for x in range(25)] + ['chr{}'.format(x) for x in range(25)] + \
                ['X', 'Y', 'M', 'chrX', 'chrY', 'chrM'])
 
    gene_size = {}
    with open(input[0], 'rU') as ifile:
        for line in ifile:
            if line.startswith('#'):
                continue
            ls = line.strip().split('\t')
            #
            # ls[0]: seqname (chromosome name)
            # ls[1]: source (--ignored--)
            # ls[2]: feature (exon only)
            # ls[3]: start position (1-based)
            # ls[4]: end position (1-based)
            # ls[5]: score (--ignored---)
            # ls[6]: strand (--ignored--)
            # ls[7]: frame (--ignored--)
            # ls[8]: attributes, look for gene_id
            #
            if ls[0] not in allowwd_chromosomes:
                continue
            if len(ls) < 5:
                continue
            if ls[2] != 'exon':
                continue
            try:
                gene_id = [x.strip().rsplit(' ', 1)[-1].strip('"').strip("'") for x in ls[8].split(';') if x.strip().startswith('gene_id')]
            except Exception as e:
                sys.stderr.write('Failed to parse {}: {}'.format(line, e))
                continue
            #
            if not gene_id:
                sys.stderr.write('No gene_id for record {}'.format(line))
            #
            # use gene_id.chr as key
            try:
                key = gene_id[0] + '.' + ls[0]
                if key in gene_size:
                    if (int(ls[3]), int(ls[4])) != gene_size[key][3][-1]:
                        gene_size[key][1] = min(gene_size[key][1], int(ls[3]))
                        gene_size[key][2] = max(gene_size[key][2], int(ls[4]))
                        gene_size[key][3].append((int(ls[3]), int(ls[4])))
                else:
                    gene_size[key] = [ls[0], int(ls[3]), int(ls[4]), [(int(ls[3]), int(ls[4]))]]
            except Exception as e:
                sys.stderr.write('Failed to store key {}: {}'.format(key, e))
                continue
    #
    with open(${output!r}, 'w') as ofile:
        for key in sorted(gene_size.keys()):
            val = gene_size[key]
            #
            # Remove chr from key for output
            ofile.write('{0}\t{1}\t{2}\t{3}\t{4}\n'.format(val[0], key[:-len(val[0])-1], val[1], val[2], 
                _size(val[3])))


[flagstat: provides='{filename}.bam.flagstat']
# Alignment (samtools): Generating statistics of aligned reads.
input:
    '${filename}.bam'

run:
    samtools flagstat ${input} > ${input}.flagstat
 
## # format of flagstat file. 
## # 
## #2181882 + 0 in total (QC-passed reads + QC-failed reads)
## #0 + 0 duplicates
## #2181882 + 0 mapped (100.00%:nan%)
## #2181882 + 0 paired in sequencing
## #mapped_reads = ${eval([line.split('m')[0] for line in open(input[0] + '.flagstat').read().split('\n') if 'mapped' in line][0])}
## 

[mapped_reads: shared='mapped_reads']
input:
    '${accepted_hits}.flagstat'

depends:
    sos_variable('accepted_hits')

with open(input[0]) as flgs:
    for line in flgs:
        if 'mapped' in line:
            mapped_reads = eval(line.split('m')[0])
            break


[human_hg19_620 (Gene Count RPKM)]

input:
     '${count_out}/${samplename}-gene_counts.tsv'

depends: 
    genesize_txt,
    sos_variable('mapped_reads')

output:
    "${count_out}/${samplename}-gene_counts_rpkm.xls"

python:
    import sys
    gene_size = {}
    ambiguous_size = set()
    with open(${depends[0]!r}) as genesize:
        for line in genesize:
            chr, gene, start, stop, size = line.split('\t')
            if gene in gene_size and gene_size[gene][3] != int(size):
                ambiguous_size.add(gene)
            gene_size[gene] = (chr, start, stop, int(size))
            gene_size[gene + '.' + chr] = (chr, start, stop, int(size))
    #
    print('Size of {} genes imported.'.format(len(gene_size)/2))
    with open(${input!r}) as counts, open(${output!r}, 'w') as output:
        output.write('Chr\tGeneID\tStart\tStop\tCodingLength\tReadCount\tRPKM\n')
        for line in counts:
            # '__no_feature', '__ambiguous', '__too_low_aQual', 'not_aligned', '__alignment_not_unique'
            if line.startswith('__'):
                continue
            try:
                gene, value = line.split('\t', 1)
            except:
                sys.stderr.write('Unprocess line: {}'.format(line))
            if not gene.strip():
                continue
            elif gene not in gene_size:
                sys.stderr.write('Unknown size of gene {}'.format(gene))
                output.write('NA\t{}\tNA\tNA\tNA\t{}\tNA\n'.format(gene, value.strip()))
            else:
                if gene in ambiguous_size:
                    sys.stderr.write('{} appear in different chromosomes. Its RPKM value is based on the size of one of them'.format(gene))
                output.write('{}\t{}\t{}\t{}\t{}\t{}\t{:.2f}\n'
                    .format(gene_size[gene][0],
                        # output gene name if the genename was appended with chromosome name
                        gene[:-len(gene_size[gene][0])-1] if gene.endswith('.' + gene_size[gene][0]) else gene,
                        gene_size[gene][1],
                        gene_size[gene][2], gene_size[gene][3], value.strip(),
                        1.e9*int(value)/(${mapped_reads} * gene_size[gene][3])))

[human_hg19_630]
#Count: Running bedtools interectBed to get exon count.
#Count: Calculate RPKM for exon counts.
input:
    sorted_bam

output:
    '${count_out}/${samplename}-exon_counts.tsv'

run:
    coverageBed -abam ${input} -b ${genes_exon_bed}  \
        > ${count_out}/${samplename}-exon_counts.tsv

[human_hg19_631]

output:
    "${count_out}/${samplename}-exon_counts_rpkm.xls"
    
depends:
    sos_variable('mapped_reads')

python:
    import sys
    from collections import defaultdict

    exon_counts = defaultdict(int)
    with open(${input!r}) as counts:
        for line in counts:
            fields = line.split()
            if len(fields) == 16:
                exon_counts[tuple(fields[12:16])] += 1
            elif len(fields) == 8:
                if int(fields[4]) != 0:
                    exon_counts[tuple(fields[0:4])] = int(fields[4])
            else:
                sys.stderr.write('Incorrect input line: {}'.format(line))
                continue
    sys.stderr.write('Writing counts for {} exons'.format(len(exon_counts)))
    with open(${output!r}, 'w') as output:
        output.write('Chr\tStart\tStop\tExon\tReadCount\tRPKM\n')
        for exon in sorted(exon_counts.keys(), key=lambda x: x[3]):
            count = exon_counts[exon]
            # the input files are in BED format with 0-based start position
            # we need to add 1 to the start position for consistency
            output.write('{}\t{}\t{}\t{}\t{}\t{:.2f}\n'
                .format(exon[0], int(exon[1]) + 1, exon[2], exon[3], count,
                        1.e9*count/(${mapped_reads} * (int(exon[2]) - int(exon[1])))))



[update_blast: provides='${ncbi_resource_dir}/blast/db/human_genomic.nal']
#Resource: Download NCBI Blast databases and link the directory
# to output dir for blast to work
depends:
    executable('update_blastdb.pl')

output:
    '${ncbi_resource_dir}/blast/db/human_genomic.nal',
    '${ncbi_resource_dir}/blast/db/nt.nal'

task:

sh: workdir='${ncbi_resource_dir}/blast/db'
    update_blastdb.pl human_genomic --decompress || true
    update_blastdb.pl nt --decompress || true

sh: workdir=output_dir
    [ -d blast ] || ln -s ${ncbi_resource_dir}/blast/db blast


[before_tophat_fusion: provides=['${output_dir}/refGene.txt', '${output_dir}/ensGene.txt']]
#Fusion (prepare): Create an alias for tophat output directory because
#    tophat-fusion-post require the directory to have name tophat-XXX
#Fusion (prepare): Link refGene.txt and ensGene.txt to output directory to be used by tophat-fusion-post.

depends:
    refgene_txt, ensgene_txt

run:    workdir=output_dir
    [ -f refGene.txt ] || ln -s ${refgene_txt} refGene.txt
    [ -f ensGene.txt ] || ln -s ${ensgene_txt} ensGene.txt

stop_if(alignment_out.rstrip('/').rsplit('/', 1)[-1].startswith('tophat_') \
    or os.path.isdir(output_dir + '/tophat_' + alignment_out.rsplit("/",1)[-1]))

run:    workdir=output_dir
    ln -s ${alignment_out.rstrip("/").rsplit("/",1)[-1]} ${'tophat_' + alignment_out.rsplit("/",1)[-1]}


[human_hg19_710]
# tophat-fusion for fusion detection
output:
    "${fusion_out}/potential_fusion.txt"

depends:
    '${output_dir}/refGene.txt',
    '${output_dir}/ensGene.txt',
    '${ncbi_resource_dir}/blast/db/human_genomic.nal'

run: workdir=output_dir

    tophat-fusion-post -p 3  -o ${fusion_out.rstrip("/").rsplit("/")[-1]} \
        ${reference_dir}/BowtieIndex/genome


[human_hg19_730]
# (Oncofuse): filter fusion candidates using Oncofuse

output:
    '${fusion_out}/fusions_oncofuse.xls'

run:
    java -Xmx1G -jar ${oncofuse_path}/Oncofuse.jar \
        ${input} tophat-post AVG ${fusion_out}/fusions_oncofuse.xls

[download_gatk: provides=gatk_resource_dir + '/dbsnp_138.hg19.vcf']
download: dest_dir=gatk_resource_dir
    ${gatk_url}/1000G_omni2.5.hg19.sites.vcf.gz
    ${gatk_url}/1000G_omni2.5.hg19.sites.vcf.gz.md5
    ${gatk_url}/1000G_omni2.5.hg19.sites.vcf.idx.gz
    ${gatk_url}/1000G_omni2.5.hg19.sites.vcf.idx.gz.md5
    ${gatk_url}/dbsnp_138.hg19.vcf.gz
    ${gatk_url}/dbsnp_138.hg19.vcf.gz.md5
    ${gatk_url}/dbsnp_138.hg19.vcf.idx.gz
    ${gatk_url}/dbsnp_138.hg19.vcf.idx.gz.md5
    ${gatk_url}/hapmap_3.3.hg19.sites.vcf.gz
    ${gatk_url}/hapmap_3.3.hg19.sites.vcf.gz.md5
    ${gatk_url}/hapmap_3.3.hg19.sites.vcf.idx.gz
    ${gatk_url}/hapmap_3.3.hg19.sites.vcf.idx.gz.md5

[human_hg19_800 (GATK UnifiedGenotyper)]
# Use GATK UnifiedGenotyper caller to call variants
input:
    sorted_unique

depends:
    '${input}.bai',
    sos_variable('mapped_reads')

# An option -L can be used to limit the variant call to targeted regions
# -L $ref_dir/hg19.master.gene.bed 

output:
    '${variant_out}/${samplename}-gatk.vcf'

run:
    java -Xmx16g -Xms512m -Djava.io.tmpdir=${temp_dir}  \
        -jar ${gatk_path}/GenomeAnalysisTK.jar   \
        -R ${reference_dir}/WholeGenomeFasta/genome.fa   \
        --filter_reads_with_N_cigar   \
        -T UnifiedGenotyper   \
        -I ${input}  \
        --out ${variant_out}/${samplename}-gatk.vcf  

[human_hg19_810 (Recalibrate Variants)]
# recalibrate and filter variants
output:
    '${variant_out}/${samplename}-filter.vcf'

depends:
    '${reference_dir}/WholeGenomeFasta/genome.fa',
    '${gatk_resource_dir}/hapmap_3.3.hg19.sites.vcf',
    '${gatk_resource_dir}/1000G_omni2.5.hg19.sites.vcf',
    '${gatk_resource_dir}/dbsnp_138.hg19.vcf'

run:
    java -Xmx16g -Xms512m  \
        -Djava.io.tmpdir=${temp_dir}  \
        -jar ${gatk_path}/GenomeAnalysisTK.jar   \
        -R ${reference_dir}/WholeGenomeFasta/genome.fa   \
        -T VariantRecalibrator  \
        -mode SNP   \
        -input ${input}  \
        -resource:hapmap,known=false,training=true,truth=true,prior=15.0 ${gatk_resource_dir}/hapmap_3.3.hg19.sites.vcf  \
        -resource:omni,known=false,training=true,truth=false,prior=12.0  ${gatk_resource_dir}/1000G_omni2.5.hg19.sites.vcf  \
        -resource:dbsnp,known=true,training=false,truth=false,prior=2.0  ${gatk_resource_dir}/dbsnp_138.hg19.vcf  \
        -an ReadPosRankSum -an FS   \
        -recalFile ${variant_out}/${samplename}.recal   \
        -tranchesFile ${variant_out}/${samplename}.tranches   \
        --maxGaussians ${4 if mapped_reads > 4000000 else 2} || true  \

if os.path.getsize('${variant_out}/${samplename}.recal') > 0:
    run(r'''
        java -Xmx16g -Xms512m   \
            -Djava.io.tmpdir=${temp_dir}   \
            -jar ${gatk_path}/GenomeAnalysisTK.jar   \
            -R ${reference_dir}/WholeGenomeFasta/genome.fa  \
            -T ApplyRecalibration   \
            -mode SNP   \
            -input ${input}   \
            -recalFile ${variant_out}/${samplename}.recal  \
            -tranchesFile ${variant_out}/${samplename}.tranches  \
            -o ${variant_out}/${samplename}-filter.vcf   \
            --ts_filter_level 99.0
    ''')
else:
    run(r'''
        java -Xmx16g -Xms512m  \
            -Djava.io.tmpdir=${temp_dir}  \
            -jar ${gatk_path}/GenomeAnalysisTK.jar  \
            -R ${reference_dir}/WholeGenomeFasta/genome.fa  \
            -V ${input}  \
            -l INFO -T VariantFiltration  \
            --filterExpression "FS > 20.0" --filterName FSFilter  \
            --filterExpression "ED > 5" --filterName EDFilter  \
            --filterExpression "ReadPosRankSum < -8.0" --filterName RPRSFilter  \
            --filterExpression "ReadPosRankSum > -8.0" --filterName RPRSFilter  \
            -o ${variant_out}/${samplename}-filter.vcf
    ''')

[human_hg19_900]
#RSeQC: Generate JPEG versions of the RSeQC plots to be embedded in final report 
input:
    rseqc_pdf

output:
    '${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.jpg',
    '${alignmentqc_out}/${samplename}.junctionSaturation_plot.jpg',
    '${alignmentqc_out}/${samplename}.splice_events.jpg',
    '${alignmentqc_out}/${samplename}.splice_junction.jpg',
    '${alignmentqc_out}/${samplename}.inner_distance_plot.jpg',
    '${alignmentqc_out}//${samplename}.DupRate_plot.jpg'

# we need to wait the completion of geneBody_coverage
run:
    cat ${' '.join([os.path.join(alignmentqc_out, x) for x in os.listdir(alignmentqc_out) if x.endswith('.r')])}  \
    | sed 's/pdf(\(.*\).pdf\(.\)/jpeg(\1.jpg\2, width=600, height=600/' | R --no-save


[human_hg19_120: shared='read_length']

input:
   fastq_files

import gzip
with gzip.open(input[0], 'rb') if input[0].endswith('.gz') else open(input[0], 'rb') as fastq:
    # skip the first line
    count = 0
    total_length = 0
    for index,line in enumerate(fastq):
        if index % 4 == 1:
            # get the second line
            total_length += len(line.decode().strip())
            count += 1
        if count == 100:
            break
    read_length = total_length / count



[alignment_summary: shared=('total_reads', 'used_reads', 'mapped_reads_perc', 'uniq_mapped_reads', 'uniq_mapped_reads_perc', 'mapped_pairs', 'mapped_pairs_perc', 'concordant_mapped_pairs', 'concordant_mapped_pairs_perc', 'genome_mapped_count', 'genome_mapped_perc', 'junction_mapped_count', 'junction_mapped_perc')]

depends:
    sos_variable('accepted_hits')

with open('${alignment_out}/prep_reads.info') as prep_reads:
    total_reads = 0
    used_reads = 0
    for line in prep_reads:
        if 'reads_in' in line:
            total_reads += int(line.split('=')[-1])
        if 'reads_out' in line:
            used_reads += int(line.split('=')[-1])
#
# get mapped reads from align_summary.txt
with open('${alignment_out}/align_summary.txt') as align_summary:
    uniq_mapped_reads = 0
    uniq_mapped_pairs = 0
    mapped_reads = 0
    mapped_pairs = 0
    concordant_mapped_pairs = 0
    for idx, line in enumerate(align_summary):
        if idx in (2, 6):
            #  Mapped   :    770754 (77.1% of input)
            mapped_reads += int(line.split(':')[1].split('(')[0].strip())
        if idx == 10:
            # Aligned pairs:
            mapped_pairs += int(line.split(':')[1].strip())
        if idx in (3, 7):
            uniq_mapped_reads += int(line.split(':')[1].split('(')[0].strip())
        if idx == 12:
            concordant_mapped_pairs = int(line.split()[0])
    uniq_mapped_reads = mapped_reads - uniq_mapped_reads
    uniq_mapped_reads_perc = uniq_mapped_reads * 100. / total_reads
    mapped_reads_perc = mapped_reads * 100. / total_reads
    concordant_mapped_pairs = mapped_pairs - concordant_mapped_pairs
    concordant_mapped_pairs_perc = concordant_mapped_pairs * 200.0 / total_reads
    mapped_pairs_perc = mapped_pairs * 200.0 / total_reads

# get junction reads
with open('${alignment_out}/junction.count') as junction_count:
    junction_mapped_count = int(junction_count.readline().strip())
    genome_mapped_count = mapped_reads - junction_mapped_count
    junction_mapped_perc = junction_mapped_count * 100. / total_reads
    genome_mapped_perc = genome_mapped_count * 100. / total_reads

uniq_mapped_reads = mapped_reads - uniq_mapped_reads
uniq_mapped_reads_perc = uniq_mapped_reads * 100. / total_reads
mapped_reads_perc = mapped_reads * 100. / total_reads
concordant_mapped_pairs = mapped_pairs - concordant_mapped_pairs
concordant_mapped_pairs_perc = concordant_mapped_pairs * 200.0 / total_reads
mapped_pairs_perc = mapped_pairs * 200.0 / total_reads
genome_mapped_count = mapped_reads - junction_mapped_count
junction_mapped_perc = junction_mapped_count * 100. / total_reads
genome_mapped_perc = genome_mapped_count * 100. / total_reads


[mean_depth: shared='mean_depth']

input:
    '${count_out}/${samplename}-exon_counts_rpkm.xls'

depends:
    sos_variable('read_length')

mean_depth = 0
with open(input[0]) as exons:
    exons.readline()
    total_reads = 0
    total_length = 0
    for line in exons:
        chr, start, end, name, count, rpkm = line.split()
        if int(count) < 2:
            continue
        total_reads += int(count)
        total_length += int(end) - int(start) + 1
    mean_depth = float(total_reads) * int(read_length) / total_length


[get_counts: shared=('num_genes', 'num_covered_genes', 'num_exons', 'num_variants', 'num_valid_variants', 'num_oncofuse_fusions')]

depends:
    '${count_out}/${samplename}-gene_counts_rpkm.xls',
    '${count_out}/${samplename}-exon_counts_rpkm.xls',
    '${fusion_out}/fusions_oncofuse.xls',
    '${variant_out}/${samplename}-filter.vcf'

def _lineCount(filename, matching=None):
    count = 0
    count_no_comment = 0
    count_matching = 0
    with open(filename, 'r') as ifile:
        for line in ifile:
            count += 1
            if not line.startswith('#'):
                count_no_comment += 1
            if matching is not None:
                try:
                    if line.split()[matching[0]].strip() == matching[1]:
                        count_matching += 1
                except:
                    pass
    return count, count_no_comment, count_matching

gene_count = _lineCount('${count_out}/${samplename}-gene_counts_rpkm.xls', matching=(5, '0'))
num_genes = gene_count[0] - 1
# all lines - 0 count
num_covered_genes = gene_count[0] - gene_count[2] - 1
num_exons = _lineCount('${count_out}/${samplename}-exon_counts_rpkm.xls')[0] - 1

num_oncofuse_fusions = _lineCount('${fusion_out}/fusions_oncofuse.xls')[0] - 1
variant_count = _lineCount('${variant_out}/${samplename}-filter.vcf', matching=(6, 'PASS'))

num_variants = variant_count[1]
num_valid_variants = variant_count[2]

[human_hg19_910]
#Summary: Write summary.html to report all results.

output:
    '${output_dir}/summary.html'

depends:
    [sos_variable(x) for x in (
        'read_length',
        'mean_depth',
        'mate_inner_dist',
        'mate_std_dev',
        'total_reads',
        'used_reads',
        'mapped_reads',
        'mapped_reads_perc',
        'uniq_mapped_reads',
        'uniq_mapped_reads_perc',
        'mapped_pairs',
        'mapped_pairs_perc',
        'concordant_mapped_pairs',
        'concordant_mapped_pairs_perc',
        'genome_mapped_count',
        'genome_mapped_perc',
        'junction_mapped_count',
        'junction_mapped_perc',
        'reference_genes',
        'reference_genome',
        'num_genes',
        'num_covered_genes',
        'num_exons',
        'num_variants',
        'num_valid_variants',
        'num_oncofuse_fusions')]

parameter: css = 'default.css'

if not os.path.isfile(css):
    logger.warning('{} does not exist. No CSS is used in HTML output'.format(css))
    css_style = ''
else:
    with open(css) as style:
        css_style = style.read()

with open(output[0], 'w') as summary:
    summary.write(r'''
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html; charset=ISO-8859-1"
    http-equiv="content-type">
  <title>${samplename}</title>
 <style type="text/css">
 ${css_style}
 </style>
</head>
<body>
<h1>RNA Seq analysis for sample ${samplename}</h1>
<p>Generated by pipeline ${pipeline_name} ${pipeline_version}</p>

<h2>Project description</h2>

The goal of the analysis is to evaluate how well RNA Seq sequencing worked on the sample,
obtain gene and exon expression, fusion candidates, and variant calls from the sample.

<h3>Basic information</h3>

<table class="gridtable">
<tr>
<th>Sample name</th><td>${samplename}</td>
</tr>
<tr>
<th>Type</th><td>Paired ends</td>
</tr>
<tr>
<th>Read length</th><td>${read_length}</td>
</tr>
<tr>
<th>Mean Depth of Coverage <sup>*</sup></th><td>${mean_depth:.1f} </td>
</tr>
<tr>
<th>Inner distance between paired reads<sup>**</sup></th><td>${mate_inner_dist} (stdev=${mate_std_dev})</td>
</tr>
</table>

<p>* The mean depth of coverage is roughly estimated by number of mapped reads times read length and divided by the total length of exons. 
A low depth of coverage might appear if the mapping rate is low.</p>
<p>** Mean and standard deviation of inner distance is estimated by distances between 1M pairs of aligned reads.</p>

<h2>Result summary</h2>

<h3>Read QC -- FastQC report</h3>
<p><a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/">FastQC</a> is a quality control tool for high throughput sequence data. 
Please check <a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/">here</a> for details of each plot.
Note that it is normal for RNA Seq samples to have high duplication level and uneven distribution of Kmer content. Warnings 
on items such GC content distribution are more alarming.</p>

<table class="gridtable">
<tr>
<tr><th>Per base sequence quality</th><th>Per base sequence content</th><th>Per sequence GC content</th></tr>
<tr>
    <td>
    <a href="../${readqc_out}/${samplename}_fastqc.html#M1">
    <IMG SRC="../${readqc_out}/per_base_quality.png" WIDTH=300 ALT="Per base sequence quality">
    </a></td>
    <td>
     <a href="../${readqc_out}/${samplename}_fastqc.html#M4">
    <IMG SRC="../${readqc_out}/per_base_sequence_content.png" WIDTH=300 ALT="Per base sequence content">
    </a></td>
    <td>
    <a href="../${readqc_out}/${samplename}_fastqc.html#M5">
    <IMG SRC="../${readqc_out}/per_sequence_gc_content.png" WIDTH=300 ALT="Per sequence GC content">
    </a></td>
</tr>
</table>

<p><a href="../${readqc_out}/${samplename}_fastqc.html">Complete FastQC reports</a> </p>

<h3>Alignment summary</h3>
<p>Input reads are aligned to the transcriptome of ${reference_genes} genes using reference genome ${reference_genome}.
The mapping rate might appear higher if the reads are mapped to the whole genome.</p>

<table class="gridtable">
<tr>
<th>Total reads</th>
<th>Used reads</th>
<th>Mapped reads</th>
<th>Uniquely mapped reads</th>
<th>Mapped pairs</th>
<th>Concordant mapped pairs</th>
<th>Mapped reads (genome)</th>
<th>Mapped reads (junction)</th>
</tr>
<tr>
<td>${total_reads:,}</td>
<td>${used_reads:,}</td>
<td>${mapped_reads:,} (${mapped_reads_perc:.1f}%)</td>
<td>${uniq_mapped_reads:,} (${uniq_mapped_reads_perc:.1f}%)</td>
<td>${mapped_pairs:,} (${mapped_pairs_perc:.1f}%)</td>
<td>${concordant_mapped_pairs:,} (${concordant_mapped_pairs_perc:.1f}%)</td>
<td>${genome_mapped_count:,} (${genome_mapped_perc:.1f}%)</td>
<td>${junction_mapped_count:,} (${junction_mapped_perc:.1f}%)</td>
</tr>
</table>

<h3>Alignment QC -- RSeQC plots</h3>
<table class="gridtable">
<tr>
<tr><th>Junction saturation</th><th>Inner distance</th><th>Splicing junction</th></tr>
<tr>
    <td>
    <a href="../${alignmentqc_out}/${samplename}.junctionSaturation_plot.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.junctionSaturation_plot.jpg" WIDTH=300 ALT="Junction saturation plot">
    </a></td>
    <td>
     <a href="../${alignmentqc_out}/${samplename}.inner_distance_plot.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.inner_distance_plot.jpg" WIDTH=300 ALT="Inner distance plot">
    </a></td>
    <td>
    <a href="../${alignmentqc_out}/${samplename}.splice_junction.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.splice_junction.jpg" WIDTH=300 ALT="Splicing junction plot">
    </a></td>
</tr>
<tr><th>Read duplication</th><th>Gene body coverage</th><th>Splicing events</th></tr>
<tr>
    <td>
    <a href="../${alignmentqc_out}/${samplename}.DupRate_plot.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.DupRate_plot.jpg" WIDTH=300 ALT="Read duplication plot">
    </a></td>
    <td>
    <a href="../${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.jpg" WIDTH=300 ALT="Gene body coverage plot">
    </a></td>
    <td>
    <a href="../${alignmentqc_out}/${samplename}.splice_events.pdf">
    <IMG SRC="../${alignmentqc_out}/${samplename}.splice_events.jpg" WIDTH=300 ALT="Splicing events plot">
    </a></td>
    </tr>
</table>

<h3>Gene and exon counts</h3>

<ul>
<li><a href="../${count_out}/${samplename}-gene_counts_rpkm.xls">Gene counts with RPKM </a> (${num_genes:,} genes, ${num_covered_genes:,} overlap with at least 1 read)</li>
<li><a href="../${count_out}/${samplename}-exon_counts_rpkm.xls">Exon counts with RPKM </a> (${num_exons:,} exons overlapped with at least 1 read)</li>
</ul>

<p>The gene count was performed by <a href="http://www-huber.embl.de/HTSeq/doc/count.html">htseq-count</a> under
the <b>union</b> model. The exon count was performed by <a href="https://code.google.com/p/bedtools/">bedtools</a>
which counts the number of reads that overlap with each exon. These files are tab-delimited files with .xls extension.</p>

<h3>Fusion candidates</h3>
The following are fusion candidates produced by tophat-fusion:
<ul>
<li><a href="../${fusion_out}/fusions_oncofuse.xls">Oncofuse filtered fusion candidates</a> (${num_oncofuse_fusions:,} fusions in tab-delimited file with .xls extension)</li>
</ul>

<h3>Variant calls</h3>
<ul>
<li> <a href="../${variant_out}/${samplename}-filter.vcf">GATK detected SNV in VCF format</a>(${num_variants:,} variants, ${num_valid_variants:,} passed basic filtering)</li>
</ul>

The variants are called using the GATK Unified Genotyper and include only Single Nucleotide Variants, no indel detection was performed.</p>
<p>
<p>
</body>
</html>
''')

[human_hg19_1000]
#Summary: Compress all result files into a deliverable package.
input:
    '${output_dir}/summary.html',
    '${readqc_out}/${samplename}_fastqc.html',
    '${readqc_out}/per_base_quality.png',
    '${readqc_out}/per_base_sequence_content.png',
    '${readqc_out}/per_sequence_gc_content.png',
    '${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.pdf',
    '${alignmentqc_out}/${samplename}.junctionSaturation_plot.pdf',
    '${alignmentqc_out}/${samplename}.splice_events.pdf',
    '${alignmentqc_out}/${samplename}.splice_junction.pdf',
    '${alignmentqc_out}/${samplename}.inner_distance_plot.pdf',
    '${alignmentqc_out}//${samplename}.DupRate_plot.pdf',
    '${alignmentqc_out}/${samplename}.geneBodyCoverage.curves.jpg',
    '${alignmentqc_out}/${samplename}.junctionSaturation_plot.jpg',
    '${alignmentqc_out}/${samplename}.splice_events.jpg',
    '${alignmentqc_out}/${samplename}.splice_junction.jpg',
    '${alignmentqc_out}/${samplename}.inner_distance_plot.jpg',
    '${alignmentqc_out}/${samplename}.DupRate_plot.jpg',
    '${count_out}/${samplename}-gene_counts_rpkm.xls',
    '${count_out}/${samplename}-exon_counts_rpkm.xls',
    '${fusion_out}/fusions_oncofuse.xls',
    '${variant_out}/${samplename}-filter.vcf'

output:
    '${output_dir}/${samplename}.zip'

run:
    zip ${output_dir}/${samplename}.zip ${input}


{% endhighlight %}

## Workflow to install required tools

```python
#!/usr/bin/env sos-runner
#fileformat=SOS1.0

#
# Auxiliary steps to install required NGS tools if it is not avaible
#

# default installation directory. This directory is recognized by
# default by SoS so that commands installed would be immediately usable
# by sos workflows. You can use commands such as
#
#    sudo sos run install_ngs install_fastqc --install-dir /usr/local/bin/
#
# to install programs in a different directory.

parameter: install_dir  = '~/.sos/bin'
parameter: download_dir = '~/.sos/bin/download'

[install_fastqc: provides=executable('fastqc')]
ver = 'v0.11.5'

download: dest_dir=download_dir
    http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_${ver}.zip

warn_if(os.path.isdir('${install_dir!e}/FastQC_${ver}'),
	"Replacing FastQC version ${ver} under ${install_dir!e}/FastQC_${ver}.")

warn_if(os.path.isfile('${install_dir!e}/fastqc}'),
	"Replacing fastqc command ${install_dir!e}/fastqc.")

run:    
    cd ${install_dir!e} && \
    rm -rf ${install_dir!e}/FastQC_${ver} && \
	rm -f ${install_dir!e}/fastqc && \
    unzip -o ${download_dir!e}/fastqc_*.zip  && \
    mv ${install_dir!e}/FastQC ${install_dir!e}/FastQC_${ver} && \
    chmod 755 ${install_dir}/FastQC_${ver}/fastqc && \
    ln -s ${install_dir}/FastQC_${ver}/fastqc ${install_dir}
	
[install_bowtie: provides=executable('bowtie')]
download: dest_dir=download_dir
    https://sourceforge.net/projects/bowtie-bio/files/bowtie/1.1.2/bowtie-1.1.2-macos-x86_64.zip

run:    workdir=download_dir
    unzip bowtie-1.1.2-macos-x86_64.zip && \
	rm -rf ${install_dir!e}/bowtie-1.1.2 && \
    mv bowtie-1.1.2 ${install_dir!e} && \
	ln -s -f ${install_dir!e}/bowtie-1.1.2/* ${install_dir!e}

[install_bowtie2: provides=executable('bowtie2')]
download: dest_dir=download_dir
    https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/bowtie2-2.2.9-macos-x86_64.zip

run:    workdir=download_dir
    unzip bowtie2-2.2.9-macos-x86_64.zip && \
	rm -rf ${install_dir!e}/bowtie2-2.2.9 && \
    mv bowtie2-2.2.9 ${install_dir!e} && \
	ln -s -f ${install_dir!e}/bowtie2-2.2.9/* ${install_dir!e}


[install_tophat2: provides=executable('tophat2')]
# install tophat 2, which requires bowtie2
#
# NOTE: The pre-compiled binary does not work well under mac so it is
# highly recommened that you compile from source. However, compiling from
# source requires xcode and boost, and the later is not among the easiest
# software package to install

depends:  executable('bowtie2')

#download: dest_dir=download_dir
#	https://ccb.jhu.edu/software/tophat/downloads/tophat-2.0.13.tar.gz

#run:	workdir=download_dir
#	tar zxf tophat-2.0.13.tar.gz && \
#	cd tophat-2.0.13 && \
#	./configure --prefix=${os.path.dirname(install_dir)!e} && \
#	make -B && make install
	
download: dest_dir=download_dir
    https://ccb.jhu.edu/software/tophat/downloads/tophat-2.0.13.OSX_x86_64.tar.gz

run:    workdir=download_dir
    tar zxf tophat-2.0.13.OSX_x86_64.tar.gz && \
	rm -rf ${install_dir!e}/tophat-2.0.13.OSX_x86_64 && \
    mv tophat-2.0.13.OSX_x86_64 ${install_dir!e} && \
	ln -s -f ${install_dir!e}/tophat-2.0.13.OSX_x86_64/* ${install_dir!e}


[install_rseqc: provides=executable('inner_distance.py')]

run:
	pip install RSeQC

```
